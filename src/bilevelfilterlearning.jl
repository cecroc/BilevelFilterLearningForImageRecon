include("systemmatrix.jl")
#include("gm_restart.jl")
include("regularizers.jl")
include("inplacefunctions.jl")

using Pkg
Pkg.add(url="https://github.com/cecroc/MIRT.jl")  # TODO: merge with MIRT branch
using MIRT #: new_pogm_state, pogm_restart!, reinit_pogm_state!, pogm_state

using LinearAlgebra
using LinearMaps
using ImageFiltering # provides the convolution function
using OffsetArrays 	# all filters will be OffsetArrays to preserve indices
# TODO: Flux is the packages causing the warning about OffsetArrays...
import Flux # import (not `using') to avoid Ïƒ name conflict!
using InplaceOps
import IterativeSolvers: cg!, CGStateVariables

#=
top level:
    argmin_(Î³) â„“(Î³; X)
    â„“(Î³; X) = âˆ‘_l 1/2||xtrue_l - Ì‚x(s_l+n, Î³)||^2_2 "loss function"
lower level:
    Ì‚x(y,Î³) = argmin_x ðš½(x;y,Î³)
    ðœ±(x;y,Î³) = 1/2||Ax-y||^2_2 + exp(Î²_0) R(x;Î³)       "cost function"
    R(x;Î³) = âˆ‘_k exp(Î²_k) 1'Ï†(h_k âŠ› x; Ïµ)              "regularizer"
Here, Î³ can be a vector of any set of parameters, such as hk, Î±, Î²...
=#

# padding functions
getpad(h) = Pad(:circular,
            Tuple(Int.(ceil.( maximum(hcat([collect(size(h[i])) for i=1:length(h)]...)',dims=1) ./2))),
            Tuple(Int.(ceil.( maximum(hcat([collect(size(h[i])) for i=1:length(h)]...)',dims=1) ./2))))
padxj(xj,h) = padarray(xj, getpad(h))
function padxj!(res, xj::AbstractArray, padval)
    # this allocates some memory, but it's fairly efficient and it's easy to read
    ba = BorderArray(xj, padval)
    ##res .= ba # equivalent to copy!(res, ba)
	copy!(res, ba)
end
function padx!(res,y,h; padval=getpad(h))
    for l = 1:length(y)
        padxj!(res[l],y[l],padval)
    end
end

# functions to create and reverse h with the correct indeces
"""
Takes in a single n-dimensional matrix (a filter) and returns the OffsetArray
for the zero-centered filter.

Examples:
- ``setoffsets([1,-1])`` returns an OffsetArray with indices 0:1
- ``setoffsets(zeros(3,3))`` returns an OffsetArray with indices -1:1 Ã— -1:1
"""
setoffsets(hk) = OffsetArray(hk, Tuple([-Int(ceil((N-2)/2)):Int(floor(N/2))
				for N in size(hk)]))
function revhk(h)
    for d = 1:ndims(h)
        h = reverse(h, dims=d)
    end
    return OffsetArray(h, Tuple([-Int(floor(N/2)):Int(ceil((N-2)/2)) for N in size(h)]) )
end
revh = (h) -> [revhk(hk) for hk in h]

# in-place direct convolution function
conv!(zjk, xjpad, hk) = imfilter!(zjk,xjpad,(hk,),NoPad(),Algorithm.FIR())

# set the offsets for a tuning parameter based on the number of filters (K)
function _getvalidÎ²(Î²,K)
    if length(Î²) == 1;
        Î² = [Î²; zeros(K)];
    elseif length(Î²) == K
		Î² = [0; Î²]
	else
        @assert length(Î²) == K+1 "Î² must be of length 1, K, or K+1"
    end
    Î² = OffsetArray(Î²,0:K)
    return Î²
end

# structure to hold pointers to temporary variables
# (these are arrays we only want to allocate once)
struct ScratchBuffer
    x1
    x2
    x3
    xpad1
    xpad2
    padval
	y1
	y2
end
scratch(x,h;y=x) = ScratchBuffer(deepcopy(x), deepcopy(x), deepcopy(x),
                        padxj(x,h), padxj(x,h), getpad(h),
						deepcopy(y), deepcopy(y))

mutable struct Hyperparameters
	h::Vector 			# Array of OffsetArrays (length K)
	Î²::OffsetArray		# vector of tuning parameters (indices [0..K])
	Î±::Vector			# vector of coefficient vectors (length K)
	B::Vector 			# Vector of basis elements (length K)
	hrev
	K::Int
end
initÎ³(; h=[], Î²=0, Î±=[], B=[]) = Hyperparameters([setoffsets(hk) for hk in h],
	_getvalidÎ²(Î²,length(h)), Î±, B, revh(h), length(h))
function validateÎ³!(Î³) # TODO generalize for Î±, B option
	Î³.K = length(Î³.h)
	Î³.hrev = revh(Î³.h)
	@assert length(Î³.Î²) == Î³.K + 1
end
function copyÎ³(Î³)
	return initÎ³(;h=[deepcopy(hk) for hk in Î³.h],
		Î²=deepcopy(Î³.Î²), Î±=deepcopy([Î³.Î±k for Î±k in Î³.Î±]),
		B=deepcopy([Î³.Bk for Bk in Î³.B]))
end

# TODO - code does not currently support non-zero âˆ‡Î³loss
# would need to correct dimensions throughout for that
# and probably define âˆ‡hkloss, âˆ‡Î²kloss, and âˆ‡Î±kloss separately
struct LossFunction
	loss::Function 		# (xhat, xtrue, Î³) -> loss function value
	âˆ‡xloss::Function 	# (xhat, xtrue, Î³) -> âˆ‡_x l(x,Î³) evaluated at xhat, s, Î³
	âˆ‡xloss!::Function 	# in-place version with inputs: (res, xhat, xtrue, Î³)
	âˆ‡Î³loss::Function  	# (xhat, xtrue, Î³) -> âˆ‡_Î³ l(x,Î³) evaluated at xhat, s, Î³
end
function _âˆ‡xloss!(dloss,xhat,xtrue) # in-place version of d_loss
    minus!(dloss, xhat, xtrue)
end
initâ„“(; loss = (xhat,xtrue,Î³)-> 1/2*sosdiff(xhat,xtrue),
		âˆ‡xloss = (xhat,xtrue,Î³) -> xhat-xtrue,
		âˆ‡xloss! = (res,xhat,xtrue,Î³) -> minus!(res, xhat, xtrue),
		âˆ‡Î³loss = (xhat,xtrue,Î³) -> 0) =
		LossFunction(loss, âˆ‡xloss, âˆ‡xloss!, âˆ‡Î³loss)

# TODO - could add a term that only takes Î³ or add Î³ to the data-fit term
# and give it a new name. Then would need to modify _d2Î¦dxdh!
struct CostFunction
	datafit::Function # (x,y;Ax) -> data-fit cost (real, non-neg)
	ddatafit::Function # (x,y;Ax) -> d(datafit)/dx
	ddatafit!::Function # (res,x,y;Ax) in-place version of ddatafit
	A 	# System matrix (LinearMap or Matrix)
	At 	# Adjoint system matrix for A' operation
	LC 	# a Lipschitz constant for the data-fit term
	reg::Regularizer!
end
# standard 2-norm data-fit term
function _datafit(x, y, A; Ax=[]) # 1/2 || Ax-y ||_2^2
	if isempty(Ax)
        Ax = A*vec(x)
    end
	return 1/2*sosdiff(Ax,y)
end
# derivative of datafit for common 2-norm data-fit term
function _ddatafit!(res, x, y, A, At; Ax=[], ytemp=similar(y))
    if isempty(Ax)
        Ax = A*vec(x)
    end
	if isempty(ytemp)
		ytemp = similar(y)
	end

	minus!(ytemp, Ax, y) #  Ax-y
	mul!(res, At, vec(ytemp)) # A'(Ax-y)

	return res
end
_initðš½(x, reg, AAt; datafit  = (x,y;Ax=[])->_datafit(x,y,AAt[1];Ax),
		 ddatafit = (x,y)->_ddatafit!(similar(x),x,y,AAt[1],AAt[2]),
		 ddatafit! = (res,x,y;Ax=[],xtemp=[],ytemp=similar(y)) ->
		 			_ddatafit!(res,x,y,AAt[1],AAt[2];Ax,ytemp)) =
	CostFunction(datafit, ddatafit, ddatafit!, AAt[1], AAt[2], AAt[3], reg)
initðš½(x; reg=reg_2norm!(), kwargs...) = _initðš½(x, reg, defineA(; xdim=size(x), values(kwargs)...))

"""
Given a set of basis vectors and coefficients, form the linear combination
    Input:
        B   :   List of length K. Each B[k] is a basis vector.
        Î±   :   Linear combination coefficients, also of length K.
    Output:
        hk  :   Vector of the linear combinations. hk = âˆ‘_k Î±[k]*B[k]
"""
_hkfromÎ±k = (Bk,Î±k) -> sum([Î±k[i]*Bk[i] for i=1:length(Î±k)],dims=1)
"""
Given a set of basis vectors and coefficients, form the linear combination
    Input:
        B   :   List of length i. Each B[i] is a list of k basis vectors.
        Î±   :   List of length i. Each Î±[i] is a list of k linear coefficients.
    Output:
        h   :   List of the linear combinations. h[i] = âˆ‘_k Î±[i][k]*B[i][k]
"""
hfromÎ± = (Î³) -> [_hkfromÎ±(Î³.B[k],Î³.Î±[k]) for k=1:Î³.K]

"""
Find the Lipschitz constant for the lower-level regularizer
"""
regLC(Î³,reg) = exp(Î³.Î²[0])*reg.LdÏ†*sum([exp(Î³.Î²[k])*sum(abs.(Î³.h[k])).^2 for k=1:Î³.K])

"""
obj = _dRdx!(dx,xpad,h,reg::regularizer;
    sb=scratch(dx,h), Î²=zeros(length(h)), hrev=revh(h))

 This function finds dR/dx = exp(Î²0) âˆ‘_k exp(Î²k) Ck' dÏ†.(h_k âŠ› x) and stores
    the result in dx (in-place operation). It returns the regularization
    function value: R(x) = exp(Î²0) âˆ‘_k exp(Î²k) 1'Ï†.(hk âŠ› x)

    Input:
        x   :   value at which to evaluate the derivative (n-dim array)
        Î³   :   defines filters and tuning parameters
        ðš½  :   includes reg field, a regularizer struct that defines Ï† and dÏ†
    Output:
        obj :   R(x) = exp(Î²0) âˆ‘_k exp(Î²k) 1'Ï†.(h_k âŠ› x)

	This function still allocates some memory (such as h[k]), and the overall
		memory use will grow with K. But, it should not grow with size(x)
		and the maximum memory use at any time is pretty small.
"""
function _dRdx!(dx, x, Î³::Hyperparameters, ðš½::CostFunction; sb=scratch(dx,Î³.h))
	# set-up
	xpad = padxj!(sb.xpad2, x, sb.padval)

    # rename buffers for easier reading (zero cost)
    hconvx = sb.x1
    buf = sb.x2
    bufpad = sb.xpad1

    obj = 0.0
    fill!(dx,0)    # initialize output gradient to 0
    for k in 1:Î³.K
		eÎ²k = exp(Î³.Î²[k])
        conv!(hconvx,xpad,Î³.h[k])     	# hk âŠ› x
        copy!(buf, hconvx)            	# buffer to hold hk âŠ› x
        ðš½.reg.Ï†(hconvx);
		obj += eÎ²k*sum(hconvx) 	# obj += exp(Î²k)*sum(Ï†.(hk âŠ› x))
        ðš½.reg.dÏ†(buf) 					# buf = dÏ†.(hk âŠ› x)
        padxj!(bufpad, buf, sb.padval)
        conv!(hconvx,bufpad,Î³.hrev[k]) 	# hconvx = dÏ†.(hk âŠ› x) âŠ› htil[k]
        lmul!(eÎ²k, hconvx) 		# hconvx .*= eÎ²k;
		plus!(dx, dx, hconvx)
    end
	eÎ²0 = exp(Î³.Î²[0])
	lmul!(eÎ²0, dx) # dx .*= exp(Î³.Î²[0])
    return obj*eÎ²0
end

# This is a helper functions for finding Ì‚x:
# Stores the gradient at x in fgrad and returns the cost function value
function _f_cost_grad!(fgrad, x, y, Î³::Hyperparameters, ðš½::CostFunction, sb;
	xax=axes(sb.x1))

	# regularizer evaluation and gradient
	x = reshape(x,xax)
    obj = _dRdx!(fgrad,x,Î³,ðš½; sb)

	# calculate Ax once then use in both datafit evaluation and ddatafit
	Ax = view(sb.y2, :) #@view view(sb.ypad1, axes(y)...)[:]
	mul!(Ax, ðš½.A, vec(x)) # Ax = A* vec(x)

	# add data-fit and regularizer gradients and evaluations
	tmp = view(sb.x3, :) # will hold gradient from data-fit term
	ðš½.ddatafit!(tmp, x, y; Ax, xtemp=sb.x1, ytemp=sb.y1)
	plus!(fgrad, tmp, fgrad)
	return ðš½.datafit(x,y;Ax) + obj # data-fit + regularizer objective
end
function f_grad!(res, x, y, Î³::Hyperparameters, ðš½::CostFunction, s::pogm_state, sb)
	 s.fcostnew = _f_cost_grad!(res, x, y, Î³, ðš½, sb)
end
function gm_user_fun(s::pogm_state)
	if s.iter>0 # set cost function after gradient is computed
		s.out[s.iter] = s.fcostnew
	end
	if s.is_converged # clean up the output
		s.out = s.out[1:s.iter]
	elseif s.iter == s.niter # reached max iterations but did not converge
		s.out = s.out[1:s.iter]
		# can print out a warning here if desired
	end
end
function gm_Î¦(y, Î³, ðš½; niter=1000, x0=y, sb=scratch(x0, Î³.h; y),
	fun = (s::pogm_state) -> gm_user_fun(s),
	kwargs...) # mom=:pgm, restart=:none...

	@assert size(ðš½.A,2)==length(x0) "Must provide x0 initializzation to gm_Î¦ "*
	 	"function if size(x)!=size(y)"

	# Not tested with a prox operator or ogm.
	new_pogm_state(;
	    f_cost = (s::pogm_state) -> s.fcostnew, # set in f_grad! function
	    f_grad! = (res, x; pogm_state) -> f_grad!(res, x, y, Î³, ðš½, pogm_state, sb),
		f_L = ðš½.LC + regLC(Î³,ðš½.reg),
	    g_prox! = (res, z, c::Real; pogm_state) -> copy!(res,z), # no prox operator
	    fun = (s::pogm_state) -> fun(s),
		x0, niter,
		values(kwargs)...)
end

"""
xs, fs, rs, out = denoise_caol!(xpad,y,h,Î²,reg::regularizer; x0=y, gm_opt,
    sb)
 Find ``xÌ‚ = argmin_x ðš½(x;y,Î³)
    given ðœ±(x;y,Î³) = 1/2||y-x||^2_2 + exp(Î²0) R(x;Î³)    "cost function"
    and R(x;Î³) = âˆ‘_k exp(Î²k) 1'Ï†(h_k âŠ› x; Ïµ)             "regularizer"
 Input:
    reg is a regularizer struct that defines Ï†, dÏ†, and LdÏ†
"""
function denoise_caol!(gm_opt::pogm_state)
    pogm_restart!(gm_opt)
    return gm_opt.xnew, gm_opt.out, gm_opt
end
function denoise_caol(y,Î³,ðš½; kwargs...)
	return denoise_caol!(gm_Î¦(y, Î³, ðš½; kwargs...))
end

"""
dh_ax = f.(z^(ax)) + Ìƒh âŠ› (df.(z) .* x^(-ax)) where z = h âŠ› x
	and f is ðš½.dÏ†
    xax should be axes(x)
"""
function _d2Î¦dxdh!(dxdh, x, Î³, ðš½, k, xax, sb)
	xpad = sb.xpad2
	hk = Î³.h[k]

	padxj!(xpad, x, sb.padval)
    fill!(dxdh,zero(eltype(xpad)))
    z   = sb.x1; conv!(z,xpad,hk)        	# z = h âŠ› x (using x1 memory slot)
    fz  = sb.x2; copy!(fz,z); ðš½.reg.dÏ†(fz)     # fz  = dÏ†(h âŠ› x) (in x2 slot)
    dfz = sb.x3; copy!(dfz,z); ðš½.reg.ddÏ†(dfz) # dfz = ddÏ†(h âŠ› x) (in x3 slot)
	# now that we've used z to compute fz, dfz, use the z memory slot
	# as a buffer (z will no longer hold h âŠ› x)
    R = CartesianIndices(axes(hk))
    for (ax,i) in zip(R,1:length(hk))
        # ax loops over the axes of each element in h
        # e.g., ax might be CartesianIndex(-1,0), for which ax.I is (-1,0)
        # i loops over the vectorized index, used for building dh

		# sb.x1 = dfz .* circshift(x,-1*ax.I)
        circshift!(sb.x1, x, -1 .*ax.I); hadamard!(sb.x1,dfz)

		# sb.x1 = Ìƒh âŠ› (dfz .* circshift(x,-1*ax.I))
        padxj!(sb.xpad1, z, sb.padval)
        conv!(sb.x1, sb.xpad1, Î³.hrev[k])

        v = @view dxdh[:,i];
		plus!(v, v, sb.x1) # dxdh[:,i] += h âŠ› (dfz .* circshift(x,-1*ax.I))
		circshift!(sb.x1, fz, ax.I); # sb.x1 = (f.(z))^(ax) = f.(z^(ax))
		plus!(v, v, sb.x1) # += f.(z^(ax))
    end
	lmul!(exp(Î³.Î²[0]+Î³.Î²[k]),dxdh)
    return dxdh
end

"""
Take the derivative w.r.t. Î²k of (âˆ‡10ðš½(x;Î³)=Ì‚x-y+ e^Î²0 âˆ‘_k e^Î²k Ck' dÏ†.(Ck x))
    which is simply e^(Î²0+Î²k) Ck' dÏ†.(Ck x).
"""
function _d2Î¦dxdÎ²!(dÎ²,x,Î³,ðš½,k,sb)
    # give memory slots easier to understand names
	xpad = sb.xpad2; padxj!(xpad, x, sb.padval)
    z = sb.x2 # use the x2 memory slot to hold hk âŠ› x
    fz = sb.x1 # use the x1 memory slot to hold dÏ†.(hk âŠ› x)
    fzpad = sb.xpad1 # use this memory slot to hold padded fz

    # get the derivative
    conv!(z,xpad,Î³.h[k]) # z = hk âŠ› x
    copy!(fz, z); ðš½.reg.dÏ†(fz) # reg uses in-place operations, so result is now in fz
    padxj!(fzpad,fz,sb.padval)
    conv!(dÎ²,fzpad,Î³.hrev[k]) # res = Ck'fz = Ck' dÏ†.(Ck x)
	lmul!(exp(Î³.Î²[0]+Î³.Î²[k]), dÎ²) # dÎ² .*= exp(Î³.Î²[0]+Î³.Î²[k])
    return dÎ²
end


# TODO - this assumes that A'A is the Hessian of the data-fit term!
"""
âˆ‡20Î¦ = _grad20!(res, v, x, Î³, ðš½, sb)
res = âˆ‡20Î¦(x) * v
"""
function _grad20!(res, v, x, Î³, ðš½, sb)
	v = view(v,:); v = reshape(v,axes(x))

	z = sb.x3 # will use this to store hk âŠ› x
    vtemp = sb.x2

	xpad = sb.xpad1
	padxj!(xpad, x, sb.padval)

    fill!(res, 0)
    for k = 1:Î³.K # loop over all filters
        conv!(z, xpad, Î³.h[k])         # z = hk âŠ› x (stored in sb.x3)
        padxj!(sb.xpad2, v, sb.padval) # overwrites sb.xpad2 to be vpad
		conv!(vtemp, sb.xpad2, Î³.h[k]) # vtemp = Ck*v
        ðš½.reg.ddÏ†(z)      			   # z is updated in-place as ddÏ†(hk âŠ› x)
		hadamard!(vtemp, z) 		   # vtemp = diag(ddÏ†(hk âŠ› x)) * (Ck*v)

        padxj!(sb.xpad2, vtemp, sb.padval) # renews sb.xpad2 as vpad
        conv!(vtemp, sb.xpad2, Î³.hrev[k])  # vtemp = Ck'diag(ddÏ†(hk âŠ› x))(Ck*v)
		lmul!(exp(Î³.Î²[k]), vtemp)		  # vtemp = e^Î²k Ck'diag(ddÏ†(hkâŠ›x))*(Ck*v)
        plus!(res, res, vtemp)
    end
    # res = e^Î²0 âˆ‘_k e^Î²k Ck' diag(ddÏ†(hk âŠ› x)) (Ck*v)
	lmul!(exp(Î³.Î²[0]), res)

	# result should be res + A'Av
	tmp_y = @view sb.y1[:]; mul!(tmp_y, ðš½.A, vec(v)) 		# sb.y1 = A*vec(v)
	tmp_x = @view sb.x3[:]; mul!(tmp_x, ðš½.At, tmp_y)  	# sb.x3 = A'A*vec(v)
	# res = A'Av + e^Î²0 âˆ‘_k e^Î²k Ck'diag(ddÏ†(hk âŠ› x))Ck*v
	plus!(res, res, sb.x3)
    return res
end

# TODO - this currently modifies CG_init if it's the same size as xjhat
# it's helpful but maybe unexpected behavoir?
function ift_gradient(xjhat, xjtrue, yj, Î³, ðš½, â„“;
	sb=scratch(xjhat, Î³.h; y=yj), sb2=scratch(xjhat, Î³.h; y=yj),
	CG_N=size(ðš½.A,2), CG_init=[],
	dh = deepcopy(Î³.h), dÎ² = deepcopy(Î³.Î²))

	# H*x computes the Hessian times a vector
	# H*y = res = A'Ay + âˆ‘_k e^Î²k Ck' * diag(ddÏ†(hk âŠ› x)) * (Ck*y) = âˆ‡_{x,x} Î¦ * v
	H = LinearMap((res,v) -> _grad20!(res,v, xjhat, Î³, ðš½, sb2),
					length(xjhat), length(xjhat), issymmetric=true, isposdef=true)
	# TODO it could be positive *semi* definite... depending on A
	# but theory requires PD - maybe warn the user?

	# approximate H^-1 * (Ì‚x-xtrue)
	er = @view view(sb.xpad1, axes(xjhat)...)[:]; minus!(er, xjhat, xjtrue)
	# get the initialization for CG which will also hold the result from CG
	if isempty(CG_init)
		cgres = @view copy(xjhat)[:]
		copyto!(cgres, er) # equivalent to approximating Hessian as Identity
	elseif length(CG_init)==length(xjhat)
		cgres = @view CG_init[:]
	elseif isa(CG_init,Number)
		cgres = @view copy(xjhat)[:]
		fill!(cgres,CG_init) # 0 is default for cg!()
	else # user supplied CG_init of incorrect size
		@warn("Initialization for CG must be a float or of size(xjhat)."*
			"Invalid size "*size(CG_init)*" provided. Initializing to 0.")
		cgres = @view copy(xjhat)[:]
		fill!(cgres,0) # 0 is default for cg!()
	end
	fill!(sb.x1,0); #fill!(sb.x2,0); fill!(sb.x3, 0)
	# vec is non-allocating
	sv = CGStateVariables(vec(sb.x1), vec(sb.x2), vec(sb.x3))
	cg!(cgres, H, er; maxiter=CG_N, statevars = sv)

	for k=1:Î³.K; fill!(dh[k],0); end
	fill!(dÎ², 0)
	dxdÎ² = zeros(size(xjhat))
	dxdh = zeros(length(xjhat),length(Î³.h[1]))
	for k = 1:Î³.K # update gradient for each filter
         if length(Î³.h[k]) != size(dxdh,2) # TODO - not tested
			 dxdh = zeros(length(xjhat),length(Î³.h[k]))
		 else
			 fill!(dxdh, 0)
		 end
        _d2Î¦dxdh!(dxdh,xjhat,Î³,ðš½,k,axes(xjhat),sb)
        _d2Î¦dxdÎ²!(dxdÎ²,xjhat,Î³,ðš½,k,sb)

		dh[k] = - setoffsets(reshape(dxdh'*cgres,size(dh[k])))
		dÎ²[k] = - vec(dxdÎ²)' * cgres
    end
	return dh, dÎ², cgres
end

mutable struct BilevelState
	# input variables (generally required from user)
	xtrue # Array of clean training samples, length L
	y # Array of noisy training samples, length L
	Î³::Hyperparameters # hyperparameters
	niter # maximum number of upper-level iterations (U in paper)

	# input variables that come with reasonable defaults
	â„“::LossFunction # upper-level loss function
	ðš½::CostFunction # lower-level cost function
	fun::Function # user output function
	descend_h::Bool
	descend_Î²::Bool

	# defining the lower-level optimizer
	lloptimizer

	# defining the gradient computation and upper-level functions
	dâ„“dÎ³
	isconverged
	# TODO add a checkconvergence field for user to define their own function
	updateÎ³!

	# iternal variables - momentum coefficients and such
	iter::Int # # of completed upper-level iterations
	Î³old::Hyperparameters
	xhat # array of length l for all current reconstructed images

	# other internal variables
	fcost
	sb # [vector of] ScratchBuffers
	L # number of training signals
	dh
	dÎ²

	# dictionary for holding method-specific options
	opt::Dict

	# user output
	out
end

# Î³, niter, â„“, ðš½

function updateÎ³_ADAM!(Î³, opth, optÎ², dh, dÎ²; descend_h=false, descend_Î²=false)
	if descend_h
		[Flux.Optimise.update!(opth[k], Î³.h[k], dh[k]) for k=1:Î³.K]
	end
	if descend_Î²
		dÎ²[0] = 0 # don't descend with repect to Î²0, TODO: make optional
		Flux.Optimise.update!(optÎ², Î³.Î², dÎ²)
	end
end
function updateÎ³_gd!(Î³, Î±â„“, dh, dÎ²; descend_h=false, descend_Î²=false)
	if descend_h
		for k=1:Î³.K; minus!(Î³.h[k], Î³.h[k], Î±â„“*dh[k]); end;
	end
	if descend_Î²
		dÎ²[0] = 0 # don't descend with repect to Î²0, TODO: make optional
		minus!(Î³.Î², Î³.Î², Î±â„“*dÎ²)
	end
end
function _lloptimizer!(s::BilevelState, l::Int; Î±ðš½=[])
	copy!(s.opt["gmopt"][l].x0, s.xhat[l])
	s.opt["gmopt"][l].f_grad! = (res, x; pogm_state) -> f_grad!(res, x, s.y[l],
		s.Î³, s.ðš½, pogm_state, s.sb[l])
	s.opt["gmopt"][l].f_L = s.ðš½.LC + regLC(s.Î³,s.ðš½.reg)
	reinit_pogm_state!(s.opt["gmopt"][l]; momchanged=false)
	if !isempty(Î±ðš½) # since re-init will overwrite this setting
		s.opt["gmopt"][l].alpha = Î±ðš½
	end
	pogm_restart!(s.opt["gmopt"][l])
	return s.opt["gmopt"][l]
end

# defaults to ADAM for upper-level
function initializebilevelstate(xtrue, y, Î³;
	xhat=length(xtrue[1])==length(y[1]) ? deepcopy(y) : fill!(deepcopy(xtrue),0),
	#xhat = [reshape(ðš½.At*vec(y[l]),size(xtrue[l])) for l=1:length(y)],
	niter=100, # upper level iterations
	T = 100, # lower level iterations
	â„“=initâ„“(),
	reg=reg_2norm!(),
	ðš½=initðš½(xtrue[1]; reg, opt=:denoising),
	sb=[scratch(xtrue[l], Î³.h; y=y[l]) for l=1:length(xtrue)],
	sb2=[scratch(xtrue[l], Î³.h; y=y[l]) for l=1:length(xtrue)],
	opt = Dict{String,Any}("h"=>[Flux.ADAM() for k=1:length(Î³.h)], "Î²"=>Flux.ADAM(),
			"cgres" => [zeros(size(xl)) for xl in xtrue],
			"gmopt"=>[gm_Î¦(y[l],Î³,ðš½; x0=xhat[l], niter=T, # alpha=Î±ðš½,
					mom=:pogm, checkconvergence=s->_checkconvergence_gnorm(s; gnorm=1e-10))
					for l=1:length(xtrue)]),
	lloptimizer = _lloptimizer!,
	dâ„“dÎ³ = (s::BilevelState, l) -> ift_gradient(s.xhat[l],xtrue[l],y[l],s.Î³,s.ðš½,s.â„“;
		sb=s.sb[l], sb2=sb2[l], CG_init=s.opt["cgres"][l]),
	fun=(s,l,llres)->bileveluserfun(s,l,llres; printfreq=100),
	descend_h=true, descend_Î²=true,
	iter=0,
	updateÎ³! = (s) -> updateÎ³_ADAM!(s.Î³, s.opt["h"], s.opt["Î²"], s.dh, s.dÎ²;
							descend_h, descend_Î²),
	isconverged = (s) -> (s.iter >= s.niter),
    out = similar(Array{Any}, niter+1) # pre-allocate output of fun evals
	)

	L = length(xtrue)
    @assert L == length(y) "Must input the same number of clean and noisy signals"
	@assert descend_h | descend_Î² "Must descend on h and/or Î²"

	Î³old = initÎ³(; h=deepcopy(Î³.h), Î²=copy(Î³.Î²), Î±=Î³.Î±, B=Î³.B)

	@assert all([size(y[l]) == size(y[1]) for l=2:L]) "support for different"*
		" input sizes not yet implemented"
	# theory is easy, but need to properly change or index sb
	# to extend to the case of different sized input signals

	convflag = false
	fcost = NaN
	dh = []
	dÎ² = []

	s = BilevelState(xtrue, y, Î³,
		niter,
		â„“, ðš½,
		fun,
		descend_h, descend_Î²,
		lloptimizer,
		dâ„“dÎ³, isconverged, updateÎ³!,
		iter, Î³old, xhat, fcost, sb,
		L, dh, dÎ²,
		opt, out)

	s.fun(s,0,[]) # call user output before taking the first iteration
	return s
end

# specific for AID_BIO method
# to match the paper settings, user must provide Î±ðš½ > 0
# else it will change each upper-level iteration
function initializebilevelstate_aidbio(xtrue, y, Î³, Î±â„“, T;
	Î±ðš½=[], Ncg=length(xtrue[1]),
	xhat = length(xtrue[1])==length(y[1]) ? deepcopy(y) : fill!(deepcopy(xtrue),0),
	 kwargs...)

	return initializebilevelstate(xtrue, y, Î³;
		lloptimizer=(s::BilevelState, l::Int) -> #_lloptimizer!(s,l;Î±ðš½),
		 	denoise_caol!(gm_Î¦(y[l],s.Î³,s.ðš½;
					x0=s.xhat[l], niter=T, sb=s.sb[l], fun=gm_user_fun, alpha=Î±ðš½,
					mom=:pgm, restart=:none))[end],
		dâ„“dÎ³ = (s::BilevelState, l) -> ift_gradient(s.xhat[l],xtrue[l],y[l],s.Î³,
			s.ðš½,s.â„“; sb=s.sb[l], CG_N=Ncg, CG_init=s.opt["cgres"][l]),
		fun=function (s,l, llres)
				bileveluserfun(s,l, llres; printfreq=100)
				if s.iter == 0 && l>0 && l <= s.L
					copy!(s.opt["cgres"][l], s.xhat[l])
					# first initialization for CG is xhat
					# after that, will use previous CG result
				end
			end,
		updateÎ³! = (s) -> updateÎ³_gd!(s.Î³, Î±â„“, s.dh, s.dÎ²;
			descend_h=s.descend_h, descend_Î²=s.descend_Î²),
		opt = Dict{String,Any}(
			"cgres" => [zeros(size(xtrue[l])) for l=1:length(xtrue)],
			"gmopt"=>[gm_Î¦(y[l],Î³,ðš½;
					x0=xhat[l], niter=T, fun=gm_user_fun, alpha=Î±ðš½, mom=:pogm,
					checkconvergence=s->_checkconvergence_gnorm(s; gnorm=1e-10))
					for l=1:length(xtrue)]),
		values(kwargs)...)
end

function initializebilevelstate_ttsa(xtrue, y, Î³, Î±â„“;
	Î±ðš½=[], ðš½=initðš½(xtrue[1]; reg, opt=:denoising),
	x0 = [denoise_caol(y[l],Î³,ðš½; niter=Int(10e6),
		checkconvergence=s->_checkconvergence_gnorm(s; gnorm=1e-10))[1] for l=1:length(y)],
	Ncg=size(ðš½.A,2), kwargs...)

	return initializebilevelstate(xtrue, y, Î³; xhat=x0,
		lloptimizer=(s::BilevelState, l::Int) -> denoise_caol!(gm_Î¦(y[l],s.Î³,s.ðš½;
					x0=s.xhat[l], niter=1, # only one LL step each outer-loop
					sb=s.sb[l], fun=gm_user_fun, alpha=Î±ðš½))[end],
					# TODO - can save memory by passing xnew = xhat[l]
		dâ„“dÎ³ = (s::BilevelState, l) -> ift_gradient(s.xhat[l],xtrue[l],y[l],s.Î³,
			s.ðš½,s.â„“; sb=s.sb[l], CG_N=Ncg, CG_init=s.opt["cgres"][l]),
		fun=(s,l,llres)->bileveluserfun(s,l,llres; printfreq=100),
		updateÎ³! = (s) -> updateÎ³_gd!(s.Î³, Î±â„“, s.dh, s.dÎ²;
			descend_h=s.descend_h, descend_Î²=s.descend_Î²),
		opt = Dict{String,Any}("cgres"=>[zeros(size(xl)) for xl in xtrue]),
		values(kwargs)...)
end

function bileveluserfun(s::BilevelState, l, llres; printfreq=100)
	if mod(s.iter,printfreq)==0
	end
	if l > s.L && s.iter >= 0
		s.out[s.iter+1] = (s.iter, s.fcost)
	end
end


function bilevel_learn_H!(s::BilevelState)
    Î²1end = @view s.Î³.Î²[1:end] # helpful for gradient

	lk = ReentrantLock()

    while !s.isconverged(s) && s.iter < s.niter # outer iterations
		# prepare for the iteration
		[copy!(s.Î³old.h[k], s.Î³.h[k]) for k=1:s.Î³.K]
		copy!(s.Î³old.Î², s.Î³.Î²)

        # recreate these variables each loop, since K could change
        s.dh = deepcopy(s.Î³.h); s.dh .*= 0 # fill! doesn't work for vectors of vectors
		s.dÎ² = deepcopy(s.Î³.Î²); fill!(s.dÎ²,0)

		# compute/estimate the upper-level gradient
		s.fcost = 0

		Threads.@threads for l = 1:s.L
			#@show Threads.threadid()
			if size(s.y[1]) != size(s.y[l]) # TODO - not yet implemented
            end

			# lower-level optimization
			llres = s.lloptimizer(s, l)
			copy!(s.xhat[l], llres.xnew)

			lock(lk) do # call user function
				s.fun(s, l, llres)
			end
		end

		for l = 1:s.L # these are not (currently) thread-safe
			# evaluate upper-level and get upper-level gradient
			s.fcost += s.â„“.loss(s.xhat[l],s.xtrue[l],s.Î³)/s.L
			dhl, dÎ²l = s.dâ„“dÎ³(s, l)
			plus!(s.dh, s.dh, dhl)
			plus!(s.dÎ², s.dÎ², dÎ²l)
        end

        # take a gradient step in the upper-level parameters
		s.updateÎ³!(s)
		validateÎ³!(s.Î³)

		# update output variable for the user
		s.iter = s.iter + 1 # tracks the number of iterations *completed*
        s.fun(s, s.L+1,[])
    end
	validateÎ³!(s.Î³)
	return s.Î³, s.out, s
end
